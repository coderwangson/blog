## 线性回归  

###  什么是线性回归  

假如给定一组特征值，给定一个输出值，线性回归的目的就是找到一系列的参数，与特征值线性组合然后得到输出值。比如现在给你一组特征$x_1=1,x_2=2$，然后对应两个输出值y，其中$x_1$对应y = 1，$x_2$ 对应y为2，所以这个时候你就能找到一个参数$\theta = 1$ 这样就有 $y = \theta x$ ，这个时候再来一个新的特征值比如$x_3 =3 $ 你就可以带入这个线性方程得到对应y。所以线性方程的难点就在于怎么找到这一系列的参数。  

通过上面的简单介绍，也能看出来线性回归一般用于回归问题，也就是其输出是一个连续值，与其相反的是分类问题，其输出是一些离散值。  

### 线性回归数学推导  

如果你想了解其原理，那么学习一下数学推导是有必要的，如果你仅仅想使用这个算法，可以跳过此步。  

根据开始我们的介绍，我们明白了线性回归大概是干什么的了，下面给出其数学定义，假如一个样本有两个特征$x_1,x_2$，并且对应参数为$\theta_1,\theta_2$,然后在添加一个$\theta_0$ 作为偏移量，所以这个时候你得到的一个回归方程为：$$h_{\theta}(x^i) = \theta_0+\theta_1 x_1^i+\theta_2x_2^i$$ ，而这个只是一个你估计的量，因为$\theta$ 都会存在一些误差，而你的目的就是使得这个误差最小，假如有一个函数$y^i =h_\theta(x^i)+\xi^i $ 这个为正确值，所以你的目的就是使得误差最小，也就是对于存在的每个$y^i-h_\theta(x^i)$最小，为了更加准确一点也就是是的函数（这个也可以通过概率论中的似然估计得到，这里不展开了）：$$J(\theta) = \frac 1 2\sum_{i=1} ^{m}(y_i-h_\theta(x^i))$$ 的值最小，这里求最小值可以使用最小二乘法也可以使用梯度下降。或者改写成矩阵的形式进行求导，求极值点。  

一些符号的解释：$x_i^j$ 这个代表第 **j** 个样本的第 **i** 个特征值。$y^j$ 代表第 **j** 个样本的输出。  

### 使用sklearn进行iris鸢尾花测试  

#### 1、 首先进行库的导入：  

	import pandas as pd
	import matplotlib.pyplot as plt
	import numpy as np
	from sklearn.linear_model import LinearRegression
	from sklearn.cross_validation import train_test_split
	from sklearn import datasets  

有些可能本次没有用上只是习惯性引入了。  

#### 2、加载数据集并划分为测试集以及训练集  

	# 读取数据集
	iris = datasets.load_iris()
	# 准备特征值
	X = iris.data
	# 准备标签y
	y = iris.target
	# 交叉验证分训练集和测试集
	X_train,X_test,y_train,y_test = train_test_split(X,y,random_state = 1)  

#### 3、进行线性回归训练   

	# 进行线性回归算法，sklearn使用的是最小二乘法
	linreg = LinearRegression()
	# fit方法就是开始把数据喂入并进行训练
	linreg.fit(X_train,y_train)
	# 使用score函数把测试集喂入能测试出其准确率
	print(linreg.score(X_test,y_test))  

#### 4、可视化输出比较结果  
	
	# 进行模型评估 得到预测值
	y_pre= linreg.predict(X_test)
	# 画图看结果  画出预测值的散点以及真实值的散点 2*np.arange(len(y_pre))代表第0、1、2个样本，乘以2只是为了让点的x轴之间更加宽，易于观察
	plt.scatter(2*np.arange(len(y_pre)),y_pre) 
	plt.scatter(2*np.arange(len(y_pre)),y_test)
	# 
	plt.show()  

![这里写图片描述](https://s1.ax1x.com/2018/07/27/PUeg2t.png)


