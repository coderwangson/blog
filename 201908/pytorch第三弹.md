# pytorch第三弹

标签： `pytorch` `深度学习`

---

## 什么是深度学习  

深度学习是将输入的数据先经过多层的卷积层（利用卷积核）进行特征提取，把提取的特征利用神经网络进行分类或者回归，通过将分类或者回归的结果和数据的真是标签进行距离计算，算出来一个损失。上面这个几个步骤构成了叫做前向传播的东西。而我们的目的是为了使得损失最小，也就是使得我们预测得到的结果无限的接近真实的标签，这里用到最优化的方法，也就是整个目标变为了`min(loss)`，而在深度学习里面，通过梯度下降法来进行找最小值，也就是把前面的卷积核里面的参数看作是自变量，神经网络里面的参数也看作是自变量，即梯度下降的目的就是找到一组合适的自变量使得损失最小，这个过程叫做反向传播。  

## 深度学习五要素  

### 输入  

这里主要指的是对数据集的加载，在计算集视觉里面代表对图片的加载，把一张图片加载成一个`c*w*h`的矩阵，其中c代表图片的通道，彩色图片为3通道，灰度图片为1通道。  


Notice:对于通道以及宽高的维度在Tensoflow中和pytoch中顺序不一样，在Tensorflow中要求输入是`w*h*c`而在pytorch中则要求是`c*w*h`，所以这个要根据不同语言加载。  

在深度学习中每次一般是输入多张图片，也就是一个批次，所以每次读取的数据是一个`batch *c*w*h`的四维矩阵格式。

### 前向传播  

前向传播是深度学习里面的重头戏，在深度学习中，大家所修改的网络结构也就是指的这个前向传播的这个结构，前向传播里面基础的包括了卷积层（主要是为了提取特征），卷积层泛指一些卷积操作，激活函数，批归一化，池化等。除了卷积层，还有全连接层（也叫神经网络），主要是用来做分类器类比于传统机器学习里面的支持向量机的算法。

### 输出    

输出一般代表整个网络的输出，也就是预测值，对于分类则是输出每个类别的分数或者叫做概率，而回归问题则输出一个数值。在分类问题里面通过softmax这种函数，把原来的全连接的输出进行映射到`[0-1]`，代表概率。

### 损失函数    

损失函数也是深度学习里面的优化部分，损失函数的含义是网络的输出值和样本真实值之间的距离，所以如果能够找到一个好的表征距离的函数做损失函数则有利于提高精度，过去的方案一般用欧式距离，虽然可行，但是网络的泛化能力可能不好，所以后来提出了许多其他的损失函数，可以说一个更好的损失函数使得网络收敛更快（因为优化就是min损失函数），使得网络泛化能力强。

### 反向传播  

如果说前面的是一辆车的车架，那么反向传播就是发动机，通过反向传播能够更新参数，使得损失函数更小。并且损失函数也是涉及数学最多的地方，不够不用担心，在pytroch里面帮我们实现了自动求导，所以可以自动进行反向传播，这也是我们选择这些深度学习框架的原因，反向传播也是一个深度学习框架的必备搭配。




